{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设一个银行要评估客户信用以决定是否给客户贷款，需要从以下几个特征（features）进行评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='perceptron-1.PNG' width='40%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令向量$\\vec{x}=(x_1,x_2,\\cdots,x_d)$ 为评估客户的特征向量。各个特征值对应一个权重系数$ w_i$，权重向量为$ \\vec{w}$。现有如下评估方式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个阈值（threshold），若：\n",
    "\n",
    "（1）$ \\sum_{i=1}^d w_ix_i>threshold $，则认为客户信用等级达到要求；\n",
    "\n",
    "（2）$ \\sum_{i=1}^d w_ix_i<threshold $，则认为客户信用等级未达到要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将达到要求归为标签（label）=+1，未到达要求归为label=-1。定义为Y:{+1(good),-1(bad)}，则对于所有的线性函数$h\\in H$，可将之前的分类函数定义为：$$ h(x)=sign((\\sum_{i=1}^d w_ix_i)-threshold) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$h(x)=sign(\\cdot)$为符号函数，输出值为Y{-1,+1}；根据前面的定义，当$h(x)<0$时输出-1，当$h(x)>0$时输出+1，当$h(x)=0$时为特殊情况，之后讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述$h(x)$就是感知机的定义，其为二分类的线性分类模型。感知机对应于输入空间（特征空间，特征向量）将实例划分为正负两类的分离超平面，属于判别模型。其输入空间为连续变量（特征向量），输出空间为离散变量（判别向量Y，仅有两个取值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$h(x)$，可做以下简化：\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "h(x)&=&sign((\\sum_{i=1}^d w_ix_i)-threshold)\\\\ &=&sign((\\sum_{i=1}^d w_ix_i)+(\\underbrace{-threshold}_{\\rm w_0} )\\cdot \\underbrace{(+1)}_{\\rm x_0})\\\\ &=&sign(\\sum_{i=0}^d w_ix_i)\\\\ &=&sign(w^Tx)\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述简化的目的是将threshold并入了$\\vec w$，而$x_0=1$并入$\\vec x$，使它们都增加一维，整个向量从第0维开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，感知机函数的样子是什么样的，我们通过下面例子可以看到。假设特征向量$\\vec x \\in R^2$，只有两个维度，则感知机函数可以写为：$$h(x)=sign(w_0+w_1x_1+w_2x_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='perceptron-2.PNG' width='40%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设图中$x_1$为横轴，为$x_2$纵轴，以$h(x)=0$为感知机函数的切换点（因为$h(x)>0$时取+1，$h(x)<0$时取-1），则可知$h(x)$为一条直线$w_0+w_1x_1+w_2x_2=0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，在二维空间中，感知机的表现形式为一线性分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机算法PLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由感知机的定义可知，感知机算法的最终目的是找到一条最好的线f，其能够将所有的数据点完美分类。但现实中找到f是非常困难的，我们一般使用部分f中的数据D（训练集）来拟合一条线g，使$g(n)\\approx f(n)=y(n)$。但二维空间中的线有无数多条，怎样找才方便找到？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这便是感知机算法的核心：先确定一条线，根据其误分类点对线进行修正，直到所有数据点被完美分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='perceptron-3.PNG' width='20%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义最开始所找线的法向量为$w_0=0$，在空间中根据其误分类点进行修正，数据来源于D：\n",
    "\n",
    "对于$t=0,1,...$（t表示修正轮数）\n",
    "\n",
    "（1）找到法向量$w_t$的下一个误分类点$(x_{n(t)},y_{n(t)})$，其一定满足：$$sign(w_t^Tx_{n(t)})\\neq y_{n(t)}$$\n",
    "\n",
    "（2）改正错误，修改直线：$$w_{t+1}\\leftarrow w_t+y_{n(t)}x_{n(t)}$$\n",
    "\n",
    "（3）直到该直线不再有分类错误时算法停止，得到$w_{PLA}$\n",
    "\n",
    "以上为感知机算法（Perceptron Learning Algorithm，PLA）的基本形式，称为简单循环（naïve cycle）；除此外还有随机循环（random cycle）等形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第（2）点，我们知道感知机分类时，分类线的法向量为正类方向；所以，正分类错误点与法向量$w_t$的夹角一定大于90度，它们的内积一定小于0，因此进行更新$w_{t+1}\\leftarrow w_t+y_{n(t)}x_{n(t)}$后，由于内积加上了正值$y_{n(t)}>0$，夹角变小，改点分类改变正确，得到一条新的线$w_{t+1}$。具体如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='perceptron-4.PNG' width='20%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，对于负分类错误点，其与法向量$w_t$的夹角一定小于90度，它们的内积一定大于0，因此进行更新$w_{t+1}\\leftarrow w_t+y_{n(t)}x_{n(t)}$后，由于内积加上了负值$y_{n(t)}<0$，夹角变大，改点分类改变正确，得到一条新的线$w_{t+1}$。具体如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='perceptron-5.PNG' width='20%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过有限次错误改正，最终得到完美分类线$w_{PLA}$：\n",
    "\n",
    "<img src='perceptron-6.PNG' width='20%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但到此时我们应该思考几个问题：\n",
    "\n",
    "（1）PLA算法一定会停下吗？\n",
    "\n",
    "（2）算法停下之后一定有$g\\approx f$吗？\n",
    "\n",
    "（3）对于D以外的数据，PLA算法的准确性还能保持吗？\n",
    "\n",
    "这都是我们需要考虑的问题，并且在简单循环算法里，一些问题并不能得到解决，需要进一步优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA算法的收敛性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑之前所提到的问题（1），PLA算法停止的必要条件为：资料D是线性可分的（Linear Separable），若不是线性可分，则无法找到一条直线将数据集正确分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='perceptron-7.PNG' width='40%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设数据集D满足线性可分条件，那么PLA算法一定能够停下吗？这需要证明该算法的收敛性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）我们假设存在一条完美直线$w_f$，满足$y_n=sign(w_f^Tx_n)$，则有$$\\underset{n}{min} y_nw_f^Tx_n>0$$\n",
    "此式子表示每个数据点到分类线都有一定距离。对于第n次犯错的点，其到直线的距离满足：$$y_{n(t)}w_f^Tx_{n(t)}\\geq \\underset{n}{min}  y_nw_f^Tx_n>0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）再根据之前的更新条件，对于任何$(x_{n(t)},y_{n(t)})$，满足$$w_{t+1}\\leftarrow w_t+y_{n(t)}x_{n(t)}$$\n",
    "改写式子如下：\n",
    "$$\\begin{eqnarray*}w_f^Tw_{t+1} \n",
    "&=& w_f^T(w_t+y_{n(t)}x_{n(t)})\\\\ \n",
    "&\\geq& w_f^Tw_t+\\underset{n}{min}  y_nw_f^Tx_n (\\underset{n}{min}  y_nw_f^Tx_n>0)\\\\ \n",
    "&>& w_f^Tw_t+0 \\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上式可以看出，每次改正错误后$w_f^Tw_{t+1}$的内积会增大，而向量内积越大，则代表两个向量夹角越小，或者向量长度越大。因此，为了证明确实PLA算法得到的$w_{t+1}$与理想分类线$w_f$之间是否相近，还需对各自进行正规化后再做内积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么$w_t$是否会无限增长，PLA算法陷入无限循环呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道$w_t$只在犯错误时进行更新，且满足$$sign(w_t^Tx_{n(t)})\\neq y_{n(t)}\\Leftrightarrow y_{n(t)}(w_t^Tx_{n(t)})\\leq0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对向量自身的长度进行处理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray*}||w_{t+1}||^2&=&||w_t+y_{n(t)}x_{n(t)}||^2 \\\\&=&||w_t||^2+2y_{n(t)}(w_t^Tx_{n(t)})+||y_{n(t)}x_{n(t)}||^2 \\\\&\\leq&||w_t||^2+0+||y_{n(t)}x_{n(t)}||^2\\\\&\\leq&||w_t||^2+\\underset{n}{max}||y_nx_n||^2\\\\&\\leq&||w_t||^2+\\underset{n}{max}||x_n||^2\\end{eqnarray*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式最后两项，我们不妨设更新直线的误分类点离直线距离最大，即$x_n$最大，由于$y_n^2=1$，略去。由此可知，$w_t$的更新次数是有限次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为证明演算次数T为有限值，我们不妨设$w_f$与$w_{t+1}$的长度相近，根据前面两个推导，令$$\\rho=\\underset{n}{min}  y_nw_f^Tx_n  \\\\\n",
    "R^2=\\underset{n}{max}||x_n||^2$$\n",
    "则可往下递推：$$\\begin{eqnarray*}w_f^Tw_{t+1}  \n",
    "&\\geq& w_f^Tw_t+\\rho \\\\ \n",
    "&\\geq& w_f^Tw_{t-1}+2\\rho \\\\\n",
    "&\\geq& \\cdots \\\\\n",
    "&\\geq& w_f^Tw_0+T\\rho \n",
    "&\\geq& T\\rho \n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一项推导：\n",
    "$$\\begin{eqnarray*}||w_{t+1}||^2&\\leq&||w_t||^2+R^2 \\\\\n",
    "&\\leq&||w_{t-1}||^2+2R^2 \\\\\n",
    "&\\leq&\\cdots  \\\\\n",
    "&\\leq&||w_0||^2+TR^2 &\\leq&TR^2\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "联合两项不等式，根据向量的性质，我们可以得到：\n",
    "$$\\begin{eqnarray*}\n",
    "T\\rho &\\leq& w_f^Tw_{t+1} \\\\\n",
    "&\\leq&||w_f||\\cdot||w_{t+1}|| \\\\\n",
    "&\\leq&||w_{t+1}|| \\leq \\sqrt TR\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以有：$T^2\\rho^2 \\leq \\ TR^2 \\Rightarrow T\\leq \\frac{R^2}{\\rho^2} $\n",
    "这是两向量长度相近或相等时的情况，其中T为演算次数。考虑向量长度时，前面提到过需要将向量进行正规化，因此最后推导的结果为：\n",
    "$$ T\\leq \\frac{w_f}{||w_f||} \\cdot \\frac{w_{t+1}}{||w_{t+1}||} \\cdot \\frac{R^2}{\\rho^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道，当两向量完全相等时，有：$$\\frac{w_f}{||w_f||} \\cdot \\frac{w_{t+1}}{||w_{t+1}||}=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此可见演算次数T是一个有限次的数，证毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA 的局限性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据之前的知识，我们知道PLA算法的优势在于简单快捷，并且能对任意维度的数据进行分类。但同时PLA算法太过理想化，有一定的缺点：\n",
    "\n",
    "（1）PLA需先假设给定数据集D是线性可分，一定有一条线$w_f$能够将数据集完美分开，但现实中大部分数据都不是完美线性可分；\n",
    "\n",
    "（2）由于参数$\\rho=\\underset{n}{min}  y_nw_f^Tx_n $，因此演算次数也取决于$w_f$，不可预知有多少次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为如上局限性，考虑实际问题时，我们假设数据集D会含有噪声点（noise），即不是完美线性可分，如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='perceptron-8.PNG' width='30%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时，我们需要解决的问题就变成：找到一条近似能够将数据集分类的线，其在分类时所犯错误最少，用表达式表示如下：$$w_g \\leftarrow \\underset{w}{argmin} \\sum_{n=1}^N[y_n \\neq sign(w^Tx_n)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但这种方法在现有的技术下很难求解（NP-hard），因此我们还需要找到另一种简化的方法，称为口袋算法（Pocket Algorithm）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "顾名思义，口袋算法是在特征空间随机取一条直线，若一条线比原来的线犯错误更少，就将其放入口袋，经过人为设定的有限次演算，找出犯错误最少的线。这是PLA的简化算法，其实精准度也不算太差，但因为要不断地更新最佳分类线，运算时时间复杂度要高于PLA。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
